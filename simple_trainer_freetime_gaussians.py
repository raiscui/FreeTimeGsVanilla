"""
FreeTimeGS: Gaussian Primitives at Anytime Anywhere for 4D Scene Reconstruction

Implementation based on the FreeTimeGS paper with gsplat rendering.

================================================================================
COMPLETE EXAMPLE: Training FreeTimeGS on the Elly Dataset
================================================================================

This example shows the full pipeline from raw data to trained 4D Gaussian model.

Step 1: Generate Dense Point Clouds with RoMaV2
-----------------------------------------------
First, run RoMaV2 triangulation to create dense per-frame COLMAP models.
This step takes ~30 minutes for 100 frames and creates ~600K points per frame.

    CUDA_VISIBLE_DEVICES=0 python triangulate_romav2.py \\
        --colmap_path /data/shared/elaheh/4D_demo/elly/undistorted/sparse/0 \\
        --cam_dir /data/shared/elaheh/4D_demo/elly/undistorted/images \\
        --output_dir /data/shared/elaheh/4D_demo/elly/undistorted \\
        --frame_start 0 --frame_end 100 --frame_step 1 \\
        --k_nearest 10 --num_matches 2000 --setting fast --no_compile

This creates:
    - /data/shared/.../sparse/frame_000000/  (dense COLMAP model for frame 0)
    - /data/shared/.../sparse/frame_000001/  (dense COLMAP model for frame 1)
    - ...
    - /data/shared/.../velocity_k10_step1.npz (KNN velocity estimates)

Step 2: Train FreeTimeGS
------------------------
Now train the 4D Gaussian model. The trainer automatically loads the dense
point clouds from Step 1 and initializes Gaussians with their velocities.

    # Quick test (100 steps, ~2 minutes)
    CUDA_VISIBLE_DEVICES=0 python simple_trainer_freetime_gaussians.py default \\
        --data-dir=/data/shared/elaheh/4D_demo/elly/undistorted \\
        --result-dir=results/freetime_elly_test \\
        --start-frame=0 --end-frame=10 \\
        --max-steps=100

    # Full training (30K steps, ~2-4 hours)
    CUDA_VISIBLE_DEVICES=0 python simple_trainer_freetime_gaussians.py default \\
        --data-dir=/data/shared/elaheh/4D_demo/elly/undistorted \\
        --result-dir=results/freetime_elly \\
        --start-frame=0 --end-frame=100 \\
        --max-steps=30000

Step 3: Monitor Training
------------------------
Open TensorBoard to see training progress:

    tensorboard --logdir=results/freetime_elly/tb --port=6006

Key metrics to watch:
    - train/loss: Should decrease steadily
    - train/l1_loss: Image reconstruction quality
    - train/num_gaussians: Number of active Gaussians
    - train/gt_vs_render: Side-by-side comparison images

Step 4: Results
---------------
After training, you'll find:
    - results/freetime_elly/ckpts/     - Model checkpoints
    - results/freetime_elly/renders/   - Validation images
    - results/freetime_elly/videos/    - Time trajectory video
    - results/freetime_elly/stats/     - Evaluation metrics (PSNR, SSIM, LPIPS)

================================================================================
Key Features
================================================================================
1. 4D Gaussian primitives with: position, time, duration, velocity, scale, orientation, opacity, SH
2. Motion function: µx(t) = µx + v * (t - µt)
3. Temporal opacity: σ(t) = exp(-0.5 * ((t - µt) / s)^2)
4. 4D regularization loss with stop-gradient
5. Periodic relocation of low-opacity primitives
6. Velocity annealing scheduler
7. Initialization from multi-frame COLMAP with KNN velocity estimation

================================================================================
Usage Options
================================================================================

# Basic training with elly dataset
CUDA_VISIBLE_DEVICES=0 python simple_trainer_freetime_gaussians.py default \\
    --data-dir=/data/shared/elaheh/4D_demo/elly/undistorted \\
    --result-dir=results/freetime_elly \\
    --start-frame=0 --end-frame=100 \\
    --max-steps=30000

# Quick test run (100 steps)
CUDA_VISIBLE_DEVICES=0 python simple_trainer_freetime_gaussians.py fast \\
    --data-dir=/data/shared/elaheh/4D_demo/elly/undistorted \\
    --result-dir=/tmp/test_freetime \\
    --start-frame=0 --end-frame=10 \\
    --max-steps=100

# Full training with paper's approach (relocation only, no densification)
CUDA_VISIBLE_DEVICES=0 python simple_trainer_freetime_gaussians.py full \\
    --data-dir=/data/shared/elaheh/4D_demo/elly/undistorted \\
    --result-dir=/data/shared/elaheh/freetime_elly_full \\
    --start-frame=0 --end-frame=100 \\
    --max-steps=30000
"""

import json
import math
import os
import time
import shutil
from dataclasses import dataclass, field
from collections import defaultdict
from typing import Dict, List, Optional, Tuple, Union

import imageio
import numpy as np
import torch
import torch.nn.functional as F
import tqdm
import tyro
import yaml
from torch import Tensor
from torch.utils.tensorboard import SummaryWriter
from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure
from fused_ssim import fused_ssim
from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity

from gsplat.rendering import rasterization
from gsplat.strategy import DefaultStrategy
from gsplat.strategy.ops import _update_param_with_optimizer
from gsplat.distributed import cli

import sys
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from datasets.FreeTime_dataset import (
    FreeTimeParser,
    FreeTimeDataset,
    load_multiframe_colmap_points,
    load_single_frame_with_velocity,
    load_startframe_tracked_velocity,
)
from utils import knn, rgb_to_sh, set_random_seed


@dataclass
class FreeTimeConfig:
    """Configuration for FreeTimeGS training."""

    # Data
    data_dir: str = "data/4d_scene"
    result_dir: str = "results/freetime"
    data_factor: int = 1

    # Frame range
    start_frame: int = 0
    end_frame: int = 300
    frame_step: int = 1  # Load every N-th frame (e.g., 10 means frames 0, 10, 20, ...)

    # RoMaV2 triangulation - run before training if enabled
    run_roma: bool = False  # If True, run RoMaV2 triangulation before training
    roma_num_matches: int = 10000  # Number of RoMa matches per pair
    roma_max_pairs: int = 10  # Max camera pairs for triangulation
    roma_setting: str = "fast"  # RoMaV2 speed preset: turbo, fast, base, precise

    # Training
    max_steps: int = 30_000
    batch_size: int = 1
    eval_steps: List[int] = field(default_factory=lambda: [7_000, 15_000, 30_000])
    save_steps: List[int] = field(default_factory=lambda: [7_000, 15_000, 30_000])

    # Model
    sh_degree: int = 3
    sh_degree_interval: int = 1000
    init_opacity: float = 0.5  # Base opacity (temporal modulation reduces effective opacity)
    init_scale: float = 1.0
    # Initial temporal duration (s in log scale: actual_duration = exp(log(s)))
    # With s=0.3, temporal opacity at t=±0.5 from µt is:
    #   exp(-0.5 * (0.5/0.3)^2) ≈ 0.06 (still visible)
    # With s=0.1, temporal opacity at t=±0.5 from µt is:
    #   exp(-0.5 * (0.5/0.1)^2) ≈ 0.0 (invisible!)
    init_duration: float = 0.3  # Larger value = visible across more time

    # Loss weights (from paper)
    lambda_img: float = 0.8
    lambda_ssim: float = 0.2
    lambda_perc: float = 0.01
    lambda_reg: float = 1e-2  # 4D regularization weight (paper value)

    # Densification strategy:
    # FreeTimeGS paper uses FIXED budget N with ONLY relocation:
    #   - No split/clone/prune (DefaultStrategy DISABLED)
    #   - Only periodic relocation: move low-opacity Gaussians to high-gradient regions
    #   - This keeps N fixed throughout training
    # DefaultStrategy's pruning kills Gaussians due to temporal opacity modulation!
    use_default_strategy: bool = False  # Paper uses relocation only, no densification

    # Periodic relocation (MCMC-style, always used)
    use_periodic_relocation: bool = True
    relocation_every: int = 100
    relocation_start_iter: int = 1000  # Delay relocation to let training stabilize
    relocation_opacity_threshold: float = 0.02  # Moderate threshold for 4D (accounts for temporal modulation)
    lambda_grad: float = 0.5  # Weight for gradient in sampling score
    lambda_opacity: float = 0.5  # Weight for opacity in sampling score

    # Velocity annealing: λt = λ0^(1-t) + λ1^t
    velocity_lr_start: float = 1e-2  # λ0
    velocity_lr_end: float = 1e-4    # λ1

    # Rendering
    near_plane: float = 0.01
    far_plane: float = 1e10
    packed: bool = False
    antialiased: bool = False

    # Strategy - configured for 4D with careful pruning
    # Lower prune_opa since temporal modulation reduces effective opacity
    # Extend refine_stop_iter for longer training
    # IMPORTANT: Disable reset_every for 4D - opacity reset + pruning kills all Gaussians!
    strategy: DefaultStrategy = field(default_factory=lambda: DefaultStrategy(
        prune_opa=0.005,  # Moderate threshold for 4D
        grow_grad2d=0.0002,  # Default gradient threshold for densification
        grow_scale3d=0.01,  # Duplicate small Gaussians, split large ones
        refine_start_iter=500,  # Start densification after warmup
        refine_stop_iter=25_000,  # Continue densification longer for 4D
        reset_every=100_000,  # Effectively disabled - reset causes mass pruning in 4D!
        refine_every=100,  # Densify every 100 steps
        verbose=True,
    ))

    # Misc
    test_every: int = 8
    tb_every: int = 100
    tb_image_every: int = 200  # Log images to tensorboard every N steps
    disable_viewer: bool = True
    port: int = 8080

    # Initialization modes:
    # 1. "multiframe": Use points from ALL frames with their actual times
    # 2. "single_frame": Use ONE frame's points + velocity (experimental)
    # 3. "reference": Use reference COLMAP + random times
    # 4. "startframe": Use START frame points only, track through subsequent frames for velocity
    #                  All points at t=0 with computed velocity (recommended for tracking)
    init_mode: str = "multiframe"
    use_velocity_init: bool = True
    velocity_npz_path: Optional[str] = None  # Pre-computed velocities (for reference mode)
    knn_match_threshold: float = 0.5
    max_init_points: Optional[int] = None  # Limit initial points (None = no limit)


def create_freetime_splats_with_optimizers(
    parser: FreeTimeParser,
    cfg: FreeTimeConfig,
    device: str = "cuda",
    world_rank: int = 0,
    world_size: int = 1,
) -> Tuple[torch.nn.ParameterDict, Dict[str, torch.optim.Optimizer]]:
    """
    Create FreeTimeGS splats with 4D parameters.

    Each Gaussian has:
    - means: [N, 3] position µx
    - times: [N, 1] temporal center µt
    - durations: [N, 1] temporal spread s (log scale)
    - velocities: [N, 3] velocity v
    - scales: [N, 3] spatial scale (log scale)
    - quats: [N, 4] orientation quaternion
    - opacities: [N] opacity σ (logit scale)
    - sh0: [N, 1, 3] DC spherical harmonics
    - shN: [N, K, 3] higher-order SH coefficients

    Two initialization modes:
    - "single_frame": Use ONE frame's points + velocity from next frame (paper's approach)
    - "multiframe": Use ALL frames' points (may cause OOM with many frames)
    - "reference": Use reference COLMAP points + KNN match for velocities
    """

    if cfg.init_mode == "single_frame":
        # Paper's approach: use ONE reference frame + velocity from matching
        print(f"[FreeTimeGS] Using single-frame initialization (paper's approach)...")

        # Get parser's transform for coordinate alignment (if normalize=True)
        parser_transform = parser.transform if hasattr(parser, 'transform') else None

        init_data = load_single_frame_with_velocity(
            cfg.data_dir,
            start_frame=cfg.start_frame,
            end_frame=cfg.end_frame,
            reference_time=0.5,  # Middle of sequence - Gaussians move both directions
            max_error=2.0,
            match_threshold=0.1,
            transform=parser_transform,  # Apply coordinate alignment inside loader
        )

        points = init_data['positions']
        times = init_data['times']
        velocities = init_data['velocities']
        rgbs = init_data['colors']
        has_velocity = init_data['has_velocity']

        N = points.shape[0]
        print(f"[FreeTimeGS] Loaded {N} points from single frame")
        print(f"[FreeTimeGS] Points with velocity: {has_velocity.sum().item()} ({100*has_velocity.sum()/N:.1f}%)")

    elif cfg.init_mode == "multiframe":
        # Load 4D points directly from multi-frame COLMAP (accumulates all frames)
        print(f"[FreeTimeGS] Using multiframe COLMAP initialization...")

        # Get parser's transform for coordinate alignment (if normalize=True)
        parser_transform = parser.transform if hasattr(parser, 'transform') else None

        init_data = load_multiframe_colmap_points(
            cfg.data_dir,
            start_frame=cfg.start_frame,
            end_frame=cfg.end_frame,
            frame_step=cfg.frame_step,
            max_error=2.0,
            match_threshold=0.1,
            transform=parser_transform,  # Apply coordinate alignment inside loader
        )

        points = init_data['positions']
        times = init_data['times']
        velocities = init_data['velocities']
        rgbs = init_data['colors']
        has_velocity = init_data['has_velocity']

        N = points.shape[0]
        print(f"[FreeTimeGS] Loaded {N} points from multi-frame COLMAP")
        print(f"[FreeTimeGS] Points with velocity: {has_velocity.sum().item()} ({100*has_velocity.sum()/N:.1f}%)")

        # Velocity statistics
        vel_norms = velocities.norm(dim=1)
        has_vel = vel_norms > 0.001
        print(f"[FreeTimeGS] Velocity stats: {has_vel.sum().item()}/{N} with velocity, "
              f"max={vel_norms.max():.4f}, mean={vel_norms[has_vel].mean():.4f}" if has_vel.any() else "no velocities")

        # Optionally limit points
        if cfg.max_init_points is not None and N > cfg.max_init_points:
            print(f"[FreeTimeGS] Sampling {cfg.max_init_points} points from {N}")
            # Prioritize points with velocity
            vel_indices = torch.where(has_velocity)[0]
            no_vel_indices = torch.where(~has_velocity)[0]

            n_vel = min(len(vel_indices), cfg.max_init_points // 2)
            n_no_vel = cfg.max_init_points - n_vel

            if n_vel < len(vel_indices):
                vel_sample = vel_indices[torch.randperm(len(vel_indices))[:n_vel]]
            else:
                vel_sample = vel_indices

            if n_no_vel < len(no_vel_indices):
                no_vel_sample = no_vel_indices[torch.randperm(len(no_vel_indices))[:n_no_vel]]
            else:
                no_vel_sample = no_vel_indices

            sample_idx = torch.cat([vel_sample, no_vel_sample])
            points = points[sample_idx]
            times = times[sample_idx]
            velocities = velocities[sample_idx]
            rgbs = rgbs[sample_idx]
            has_velocity = has_velocity[sample_idx]
            N = points.shape[0]

    elif cfg.init_mode == "startframe":
        # NEW: Load points from START frame only, track through subsequent frames for velocity
        print(f"[FreeTimeGS] Using startframe + tracked velocity initialization...")
        print(f"[FreeTimeGS] This uses ONLY frame 0 points, tracked through frames to compute velocity")

        # Get parser's transform for coordinate alignment (if normalize=True)
        parser_transform = parser.transform if hasattr(parser, 'transform') else None

        init_data = load_startframe_tracked_velocity(
            cfg.data_dir,
            start_frame=cfg.start_frame,
            end_frame=cfg.end_frame,
            frame_step=cfg.frame_step,
            max_error=2.0,
            match_threshold=0.1,
            transform=parser_transform,
        )

        points = init_data['positions']
        times = init_data['times']  # All zeros (t=0)
        velocities = init_data['velocities']
        rgbs = init_data['colors']
        has_velocity = init_data['has_velocity']

        N = points.shape[0]
        print(f"[FreeTimeGS] Loaded {N} points from start frame (all at t=0)")
        print(f"[FreeTimeGS] Points with velocity: {has_velocity.sum().item()} ({100*has_velocity.sum()/N:.1f}%)")

        # Velocity statistics
        vel_norms = velocities.norm(dim=1)
        has_vel = vel_norms > 0.001
        if has_vel.any():
            print(f"[FreeTimeGS] Velocity stats: {has_vel.sum().item()}/{N} with velocity, "
                  f"max={vel_norms.max():.4f}, mean={vel_norms[has_vel].mean():.4f}")

        # Optionally limit points
        if cfg.max_init_points is not None and N > cfg.max_init_points:
            print(f"[FreeTimeGS] Sampling {cfg.max_init_points} points from {N}")
            # Prioritize points with velocity
            vel_indices = torch.where(has_velocity)[0]
            no_vel_indices = torch.where(~has_velocity)[0]

            n_vel = min(len(vel_indices), cfg.max_init_points // 2)
            n_no_vel = cfg.max_init_points - n_vel

            if n_vel < len(vel_indices):
                vel_sample = vel_indices[torch.randperm(len(vel_indices))[:n_vel]]
            else:
                vel_sample = vel_indices

            if n_no_vel < len(no_vel_indices):
                no_vel_sample = no_vel_indices[torch.randperm(len(no_vel_indices))[:n_no_vel]]
            else:
                no_vel_sample = no_vel_indices

            sample_idx = torch.cat([vel_sample, no_vel_sample])
            points = points[sample_idx]
            times = times[sample_idx]
            velocities = velocities[sample_idx]
            rgbs = rgbs[sample_idx]
            has_velocity = has_velocity[sample_idx]
            N = points.shape[0]

    else:
        # Original mode: use reference COLMAP
        print(f"[FreeTimeGS] Using reference COLMAP initialization...")
        points = torch.from_numpy(parser.points).float()
        rgbs = torch.from_numpy(parser.points_rgb / 255.0).float()
        N = points.shape[0]

        # Time: initialize uniformly in [0, 1]
        times = torch.rand((N, 1))

        # Velocity: initialize to zero
        velocities = torch.zeros((N, 3))

        print(f"[FreeTimeGS] Initializing {N} Gaussians from reference COLMAP")

        # Load pre-computed velocities if available
        if cfg.use_velocity_init and cfg.velocity_npz_path and os.path.exists(cfg.velocity_npz_path):
            print(f"[FreeTimeGS] Loading velocities from {cfg.velocity_npz_path}")
            vel_data = np.load(cfg.velocity_npz_path)

            vel_positions = torch.from_numpy(vel_data['initial_positions']).float()
            vel_velocities = torch.from_numpy(vel_data['velocities']).float()

            # Match points to velocity bank using KNN
            from scipy.spatial import cKDTree
            tree = cKDTree(vel_positions.numpy())
            distances, indices = tree.query(points.numpy(), k=1)

            valid_matches = distances < cfg.knn_match_threshold
            velocities[valid_matches] = vel_velocities[indices[valid_matches]]

            n_matched = valid_matches.sum()
            print(f"[FreeTimeGS] Matched {n_matched}/{N} points with velocities")

    # Initialize spatial scale from KNN
    dist2_avg = (knn(points, 4)[:, 1:] ** 2).mean(dim=-1)
    dist_avg = torch.sqrt(dist2_avg)
    scales = torch.log(dist_avg * cfg.init_scale).unsqueeze(-1).repeat(1, 3)

    # Distribute across ranks
    points = points[world_rank::world_size]
    rgbs = rgbs[world_rank::world_size]
    scales = scales[world_rank::world_size]
    times = times[world_rank::world_size]
    velocities = velocities[world_rank::world_size]
    N = points.shape[0]

    # Initialize other 4D parameters
    quats = torch.rand((N, 4))
    opacities = torch.logit(torch.full((N,), cfg.init_opacity))

    # Duration: initialize to cfg.init_duration (log scale for positivity)
    durations = torch.log(torch.full((N, 1), cfg.init_duration))

    # Spherical harmonics
    colors = torch.zeros((N, (cfg.sh_degree + 1) ** 2, 3))
    colors[:, 0, :] = rgb_to_sh(rgbs)

    # Build parameter list with learning rates
    scene_scale = parser.scene_scale
    params = [
        ("means", torch.nn.Parameter(points), 1.6e-4 * scene_scale),
        ("times", torch.nn.Parameter(times), 1e-3),
        ("durations", torch.nn.Parameter(durations), 1e-3),
        ("velocities", torch.nn.Parameter(velocities), cfg.velocity_lr_start),
        ("scales", torch.nn.Parameter(scales), 5e-3),
        ("quats", torch.nn.Parameter(quats), 1e-3),
        ("opacities", torch.nn.Parameter(opacities), 5e-2),
        ("sh0", torch.nn.Parameter(colors[:, :1, :]), 2.5e-3),
        ("shN", torch.nn.Parameter(colors[:, 1:, :]), 2.5e-3 / 20),
    ]

    splats = torch.nn.ParameterDict({n: v for n, v, _ in params}).to(device)

    # Create optimizers
    BS = cfg.batch_size * world_size
    optimizers = {
        name: torch.optim.Adam(
            [{"params": splats[name], "lr": lr * math.sqrt(BS), "name": name}],
            eps=1e-15 / math.sqrt(BS),
            betas=(1 - BS * (1 - 0.9), 1 - BS * (1 - 0.999)),
        )
        for name, _, lr in params
    }

    return splats, optimizers


class FreeTimeGSRunner:
    """Training runner for FreeTimeGS."""

    def __init__(
        self,
        local_rank: int,
        world_rank: int,
        world_size: int,
        cfg: FreeTimeConfig,
    ):
        set_random_seed(42 + local_rank)

        self.cfg = cfg
        self.world_rank = world_rank
        self.local_rank = local_rank
        self.world_size = world_size
        self.device = f"cuda:{local_rank}"

        # Setup directories
        os.makedirs(cfg.result_dir, exist_ok=True)
        self.ckpt_dir = f"{cfg.result_dir}/ckpts"
        os.makedirs(self.ckpt_dir, exist_ok=True)
        self.stats_dir = f"{cfg.result_dir}/stats"
        os.makedirs(self.stats_dir, exist_ok=True)
        self.render_dir = f"{cfg.result_dir}/renders"
        os.makedirs(self.render_dir, exist_ok=True)

        # Tensorboard
        self.writer = SummaryWriter(log_dir=f"{cfg.result_dir}/tb")

        # Load data
        self.parser = FreeTimeParser(
            data_dir=cfg.data_dir,
            factor=cfg.data_factor,
            normalize=True,
            test_every=cfg.test_every,
            start_frame=cfg.start_frame,
            end_frame=cfg.end_frame,
        )

        self.trainset = FreeTimeDataset(self.parser, split="train")
        self.valset = FreeTimeDataset(self.parser, split="val")
        self.scene_scale = self.parser.scene_scale * 1.1

        print(f"[FreeTimeGS] Scene scale: {self.scene_scale}")
        print(f"[FreeTimeGS] Train samples: {len(self.trainset)}")
        print(f"[FreeTimeGS] Val samples: {len(self.valset)}")

        # Create model
        self.splats, self.optimizers = create_freetime_splats_with_optimizers(
            self.parser,
            cfg,
            device=self.device,
            world_rank=world_rank,
            world_size=world_size,
        )

        print(f"[FreeTimeGS] Initialized {len(self.splats['means'])} Gaussians")

        # Strategy for densification
        self.cfg.strategy.check_sanity(self.splats, self.optimizers)
        self.strategy_state = self.cfg.strategy.initialize_state(scene_scale=self.scene_scale)

        # Losses
        self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(self.device)
        self.psnr = PeakSignalNoiseRatio(data_range=1.0).to(self.device)
        self.lpips = LearnedPerceptualImagePatchSimilarity(
            net_type="alex", normalize=True
        ).to(self.device)

    def compute_temporal_opacity(self, t: float) -> Tensor:
        """
        Compute temporal opacity σ(t) for all Gaussians at time t.

        σ(t) = exp(-0.5 * ((t - µt) / s)^2)

        Args:
            t: Query time in [0, 1]

        Returns:
            [N] tensor of temporal opacities
        """
        mu_t = self.splats["times"]  # [N, 1]
        s = torch.exp(self.splats["durations"])  # [N, 1] - duration in positive space

        # Temporal Gaussian
        temporal_opacity = torch.exp(-0.5 * ((t - mu_t) / (s + 1e-8)) ** 2)

        return temporal_opacity.squeeze(-1)  # [N]

    def compute_moved_positions(self, t: float) -> Tensor:
        """
        Compute moved positions µx(t) = µx + v * (t - µt)

        Args:
            t: Query time in [0, 1]

        Returns:
            [N, 3] tensor of moved positions
        """
        mu_x = self.splats["means"]  # [N, 3]
        mu_t = self.splats["times"]  # [N, 1]
        v = self.splats["velocities"]  # [N, 3]

        # Motion function
        dt = t - mu_t  # [N, 1]
        moved_positions = mu_x + v * dt  # [N, 3]

        return moved_positions

    def rasterize_at_time(
        self,
        camtoworlds: Tensor,
        Ks: Tensor,
        width: int,
        height: int,
        t: float,
        sh_degree: int,
        **kwargs,
    ) -> Tuple[Tensor, Tensor, Dict]:
        """
        Rasterize Gaussians at a specific time t.

        From FreeTimeGS paper:
        1. Compute moved positions: µx(t) = µx + v * (t - µt)  (Eq. 1)
        2. Compute temporal opacity: σ(t) = exp(-0.5 * ((t - µt) / s)^2)  (Eq. 4)
        3. Modulate opacity: σ_effective = σ * σ(t)  (Eq. 3)
        """

        # Motion function: µx(t) = µx + v * (t - µt)
        means = self.compute_moved_positions(t)  # [N, 3]

        # Temporal opacity: σ(t) = exp(-0.5 * ((t - µt) / s)^2)
        temporal_opacity = self.compute_temporal_opacity(t)  # [N]

        # Standard Gaussian parameters
        quats = self.splats["quats"]  # [N, 4]
        scales = torch.exp(self.splats["scales"])  # [N, 3]
        base_opacity = torch.sigmoid(self.splats["opacities"])  # [N]

        # Modulate opacity with temporal opacity (Eq. 3)
        opacities = base_opacity * temporal_opacity  # [N]

        # Spherical harmonics
        sh0 = self.splats["sh0"]
        shN = self.splats["shN"]
        colors = torch.cat([sh0, shN], dim=1)  # [N, K, 3]

        # Rasterize
        rasterize_mode = "antialiased" if self.cfg.antialiased else "classic"

        # Extract near/far plane from kwargs or use config defaults
        near_plane = kwargs.pop("near_plane", self.cfg.near_plane)
        far_plane = kwargs.pop("far_plane", self.cfg.far_plane)

        render_colors, render_alphas, info = rasterization(
            means=means,
            quats=quats,
            scales=scales,
            opacities=opacities,
            colors=colors,
            viewmats=torch.linalg.inv(camtoworlds),
            Ks=Ks,
            width=width,
            height=height,
            packed=self.cfg.packed,
            absgrad=self.cfg.strategy.absgrad if hasattr(self.cfg.strategy, 'absgrad') else False,
            rasterize_mode=rasterize_mode,
            sh_degree=sh_degree,
            near_plane=near_plane,
            far_plane=far_plane,
            **kwargs,
        )

        # Store temporal opacity for regularization
        info["temporal_opacity"] = temporal_opacity

        return render_colors, render_alphas, info

    def compute_4d_regularization(self, temporal_opacity: Tensor) -> Tensor:
        """
        Compute 4D regularization loss.

        L_reg(t) = (1/N) * Σ(σ * sg[σ(t)])

        The stop-gradient on σ(t) prevents the regularization from
        minimizing temporal opacity.
        """
        base_opacity = torch.sigmoid(self.splats["opacities"])  # [N]

        # Stop gradient on temporal opacity
        temporal_opacity_sg = temporal_opacity.detach()

        # Weighted regularization
        reg_loss = (base_opacity * temporal_opacity_sg).mean()

        return reg_loss

    def get_velocity_lr_scale(self, step: int, max_steps: int) -> float:
        """
        Compute velocity learning rate scale using annealing.

        λt = λ0^(1-t) + λ1^t, where t ∈ [0, 1]
        """
        t = step / max_steps
        lambda_0 = self.cfg.velocity_lr_start
        lambda_1 = self.cfg.velocity_lr_end

        # Annealing formula from paper
        lr_scale = (lambda_0 ** (1 - t)) * (lambda_1 ** t)

        return lr_scale / lambda_0  # Return relative scale

    def periodic_relocation(self, step: int, info: Dict):
        """
        Relocate low-opacity Gaussians to high-score regions.

        This is similar to MCMC strategy's relocation but uses a sampling score
        based on both gradient and opacity: s = λg * ∇g + λo * σ

        Without periodic relocation, the model tends to use more Gaussians
        with lower opacity to model the scene, leading to suboptimal results
        when the number of Gaussians is limited (as noted in FreeTimeGS paper).

        The relocation properly updates optimizer states for all parameters
        including 4D parameters (times, durations, velocities).
        """
        with torch.no_grad():
            base_opacity = torch.sigmoid(self.splats["opacities"])

            # Get spatial gradients if available (accumulated from strategy state)
            if self.strategy_state.get("grad2d") is not None:
                count = self.strategy_state["count"].clamp_min(1)
                spatial_grad = self.strategy_state["grad2d"] / count
            elif "means2d" in info and self.splats["means"].grad is not None:
                spatial_grad = self.splats["means"].grad.norm(dim=-1)
            else:
                spatial_grad = torch.zeros_like(base_opacity)

            # Compute sampling score: s = λg * ∇g + λo * σ
            sampling_score = (
                self.cfg.lambda_grad * spatial_grad +
                self.cfg.lambda_opacity * base_opacity
            )

            # Find low-opacity (dead) Gaussians to relocate
            dead_mask = base_opacity < self.cfg.relocation_opacity_threshold
            n_dead = dead_mask.sum().item()

            if n_dead == 0:
                return 0

            # Find alive Gaussians to sample from
            alive_mask = ~dead_mask
            if alive_mask.sum() == 0:
                return 0

            dead_indices = dead_mask.nonzero(as_tuple=True)[0]
            alive_indices = alive_mask.nonzero(as_tuple=True)[0]

            # Probability proportional to sampling score of alive Gaussians
            probs = sampling_score[alive_indices]
            probs = probs / probs.sum().clamp_min(1e-8)

            # Sample source indices from alive Gaussians
            sampled_idxs = torch.multinomial(probs, n_dead, replacement=True)
            source_indices = alive_indices[sampled_idxs]

            # Relocate: copy all parameters from source to dead Gaussians
            # This properly handles all 4D parameters (times, durations, velocities)
            def param_fn(name: str, p: Tensor) -> Tensor:
                if name == "means":
                    # Add small noise to positions
                    noise_scale = 0.01 * self.scene_scale
                    noise = torch.randn(n_dead, 3, device=self.device) * noise_scale
                    p[dead_indices] = p[source_indices] + noise
                elif name == "opacities":
                    # Reset opacity to initial value
                    p[dead_indices] = torch.logit(
                        torch.full((n_dead,), self.cfg.init_opacity, device=self.device)
                    )
                else:
                    # Copy other parameters (scales, quats, sh0, shN, times, durations, velocities)
                    p[dead_indices] = p[source_indices]
                return torch.nn.Parameter(p, requires_grad=p.requires_grad)

            def optimizer_fn(key: str, v: Tensor) -> Tensor:
                # Reset optimizer state for relocated Gaussians
                v[dead_indices] = 0
                return v

            # Update parameters and optimizer states
            _update_param_with_optimizer(
                param_fn, optimizer_fn, self.splats, self.optimizers
            )

            # Update strategy state if it exists
            for k, v in self.strategy_state.items():
                if isinstance(v, torch.Tensor) and v.shape[0] == len(base_opacity):
                    v[dead_indices] = 0

            print(f"[Relocation] Relocated {n_dead} low-opacity Gaussians at step {step}")
            return n_dead

    def train(self):
        cfg = self.cfg
        device = self.device
        max_steps = cfg.max_steps

        # Dump config
        if self.world_rank == 0:
            with open(f"{cfg.result_dir}/cfg.yml", "w") as f:
                yaml.dump(vars(cfg), f)

        # Learning rate schedulers
        schedulers = [
            torch.optim.lr_scheduler.ExponentialLR(
                self.optimizers["means"], gamma=0.01 ** (1.0 / max_steps)
            ),
        ]

        # Data loader
        trainloader = torch.utils.data.DataLoader(
            self.trainset,
            batch_size=cfg.batch_size,
            shuffle=True,
            num_workers=4,
            persistent_workers=True,
            pin_memory=True,
        )
        trainloader_iter = iter(trainloader)

        # Training loop
        global_tic = time.time()
        pbar = tqdm.tqdm(range(max_steps))

        for step in pbar:
            # Load batch
            try:
                data = next(trainloader_iter)
            except StopIteration:
                trainloader_iter = iter(trainloader)
                data = next(trainloader_iter)

            camtoworlds = data["camtoworld"].to(device)  # [B, 4, 4]
            Ks = data["K"].to(device)  # [B, 3, 3]
            pixels = data["image"].to(device) / 255.0  # [B, H, W, 3]
            t = data["time"].to(device).mean().item()  # Scalar time

            height, width = pixels.shape[1:3]

            # SH degree schedule
            sh_degree_to_use = min(step // cfg.sh_degree_interval, cfg.sh_degree)

            # Update velocity learning rate (annealing)
            vel_lr_scale = self.get_velocity_lr_scale(step, max_steps)
            for param_group in self.optimizers["velocities"].param_groups:
                param_group["lr"] = cfg.velocity_lr_start * vel_lr_scale * math.sqrt(cfg.batch_size)

            # Forward pass: render at time t
            renders, alphas, info = self.rasterize_at_time(
                camtoworlds=camtoworlds,
                Ks=Ks,
                width=width,
                height=height,
                t=t,
                sh_degree=sh_degree_to_use,
                near_plane=cfg.near_plane,
                far_plane=cfg.far_plane,
            )

            colors = renders[..., :3]

            # Strategy pre-backward (only if using DefaultStrategy)
            if cfg.use_default_strategy:
                self.cfg.strategy.step_pre_backward(
                    params=self.splats,
                    optimizers=self.optimizers,
                    state=self.strategy_state,
                    step=step,
                    info=info,
                )

            # Compute losses
            # Clamp colors to [0, 1] for loss computation (SH can produce values > 1)
            colors_clamped = torch.clamp(colors, 0.0, 1.0)

            # Image loss (L1)
            l1_loss = F.l1_loss(colors_clamped, pixels)

            # SSIM loss
            ssim_loss = 1.0 - fused_ssim(
                colors_clamped.permute(0, 3, 1, 2),
                pixels.permute(0, 3, 1, 2),
                padding="valid"
            )

            # Perceptual loss (LPIPS) - requires [0, 1] range
            lpips_loss = self.lpips(
                colors_clamped.permute(0, 3, 1, 2),
                pixels.permute(0, 3, 1, 2)
            )

            # Combined rendering loss
            render_loss = (
                cfg.lambda_img * l1_loss +
                cfg.lambda_ssim * ssim_loss +
                cfg.lambda_perc * lpips_loss
            )

            # 4D regularization loss
            reg_loss = self.compute_4d_regularization(info["temporal_opacity"])

            # Total loss
            loss = render_loss + cfg.lambda_reg * reg_loss

            # Backward
            loss.backward()

            # Progress bar
            desc = (
                f"loss={loss.item():.4f} | "
                f"l1={l1_loss.item():.4f} | "
                f"reg={reg_loss.item():.4f} | "
                f"t={t:.2f} | "
                f"N={len(self.splats['means'])}"
            )
            pbar.set_description(desc)

            # Tensorboard logging
            if self.world_rank == 0 and step % cfg.tb_every == 0:
                self.writer.add_scalar("train/loss", loss.item(), step)
                self.writer.add_scalar("train/l1_loss", l1_loss.item(), step)
                self.writer.add_scalar("train/ssim_loss", ssim_loss.item(), step)
                self.writer.add_scalar("train/lpips_loss", lpips_loss.item(), step)
                self.writer.add_scalar("train/reg_loss", reg_loss.item(), step)
                self.writer.add_scalar("train/num_gaussians", len(self.splats["means"]), step)
                self.writer.add_scalar("train/vel_lr_scale", vel_lr_scale, step)

                # Velocity statistics (guard against empty)
                if len(self.splats["velocities"]) > 0:
                    vel_mag = self.splats["velocities"].detach().norm(dim=-1)
                    self.writer.add_scalar("train/vel_mean", vel_mag.mean().item(), step)
                    self.writer.add_scalar("train/vel_max", vel_mag.max().item(), step)

                # Temporal statistics (guard against empty)
                if len(self.splats["durations"]) > 0:
                    durations = torch.exp(self.splats["durations"]).detach()
                    self.writer.add_scalar("train/duration_mean", durations.mean().item(), step)

                # Log images: rendered vs ground truth (side by side)
                if step % cfg.tb_image_every == 0:
                    rendered = colors_clamped[0].detach()  # [H, W, 3] - already clamped
                    gt = pixels[0].detach()  # [H, W, 3]

                    # Create side-by-side comparison (GT left, Rendered right)
                    comparison = torch.cat([gt, rendered], dim=1)  # [H, 2W, 3]

                    # Convert to [3, H, W] for tensorboard
                    comparison = comparison.permute(2, 0, 1).contiguous()

                    # Use consistent tag name so images appear in sequence
                    self.writer.add_image(
                        "train/gt_vs_render",
                        comparison,
                        step,
                    )
                    print(f"[TensorBoard] Logged image at step {step}, t={t:.2f}")

                self.writer.flush()

            # Save checkpoints
            if step in [i - 1 for i in cfg.save_steps] or step == max_steps - 1:
                self._save_checkpoint(step)

            # Optimizer step
            for optimizer in self.optimizers.values():
                optimizer.step()
                optimizer.zero_grad(set_to_none=True)

            for scheduler in schedulers:
                scheduler.step()

            # Strategy post-backward (densification) - only if enabled
            # FreeTimeGS paper uses fixed N with relocation only (no split/clone/prune)
            if cfg.use_default_strategy and isinstance(self.cfg.strategy, DefaultStrategy):
                n_before = len(self.splats["means"])
                self.cfg.strategy.step_post_backward(
                    params=self.splats,
                    optimizers=self.optimizers,
                    state=self.strategy_state,
                    step=step,
                    info=info,
                    packed=cfg.packed,
                )
                n_after = len(self.splats["means"])
                if n_after != n_before:
                    print(f"[Densification] Step {step}: {n_before} -> {n_after} Gaussians "
                          f"(+{n_after - n_before})" if n_after > n_before else
                          f"[Densification] Step {step}: {n_before} -> {n_after} Gaussians "
                          f"({n_after - n_before})")
                    if self.world_rank == 0:
                        self.writer.add_scalar("train/num_gaussians", n_after, step)

            # Safety check: stop if all Gaussians are pruned
            if len(self.splats["means"]) == 0:
                print(f"[FreeTimeGS] ERROR: All Gaussians pruned at step {step}!")
                print("  This usually means:")
                print("  1. init_duration is too small (Gaussians invisible at most times)")
                print("  2. Temporal opacity too low causing aggressive pruning")
                print("  Try increasing --init_duration (e.g., 0.5) or disable pruning initially")
                break

            # Periodic relocation (FreeTimeGS-specific)
            # This works alongside DefaultStrategy densification:
            # - DefaultStrategy: split/clone/prune based on gradients and opacity
            # - Periodic relocation: move low-opacity Gaussians to high-score regions
            # Without this, model uses more low-opacity Gaussians (suboptimal when N is limited)
            if cfg.use_periodic_relocation and step >= cfg.relocation_start_iter and step % cfg.relocation_every == 0:
                n_relocated = self.periodic_relocation(step, info)
                if self.world_rank == 0:
                    self.writer.add_scalar("train/n_relocated", n_relocated, step)

            # Evaluation
            if step in [i - 1 for i in cfg.eval_steps]:
                self.eval(step)

        print(f"[FreeTimeGS] Training completed in {time.time() - global_tic:.1f}s")

    def _save_checkpoint(self, step: int):
        """Save model checkpoint."""
        data = {
            "step": step,
            "splats": self.splats.state_dict(),
        }
        torch.save(data, f"{self.ckpt_dir}/ckpt_{step}_rank{self.world_rank}.pt")
        print(f"[FreeTimeGS] Saved checkpoint at step {step}")

    @torch.no_grad()
    def eval(self, step: int):
        """Evaluate on validation set."""
        print(f"[FreeTimeGS] Running evaluation at step {step}...")

        cfg = self.cfg
        device = self.device

        valloader = torch.utils.data.DataLoader(
            self.valset, batch_size=1, shuffle=False, num_workers=1
        )

        metrics = defaultdict(list)
        render_times = []

        for i, data in enumerate(valloader):
            camtoworlds = data["camtoworld"].to(device)
            Ks = data["K"].to(device)
            pixels = data["image"].to(device) / 255.0
            t = data["time"].to(device).mean().item()
            height, width = pixels.shape[1:3]

            torch.cuda.synchronize()
            tic = time.time()

            colors, _, _ = self.rasterize_at_time(
                camtoworlds=camtoworlds,
                Ks=Ks,
                width=width,
                height=height,
                t=t,
                sh_degree=cfg.sh_degree,
                near_plane=cfg.near_plane,
                far_plane=cfg.far_plane,
            )

            torch.cuda.synchronize()
            render_times.append(time.time() - tic)

            colors = torch.clamp(colors[..., :3], 0.0, 1.0)

            # Save rendered images
            if self.world_rank == 0:
                canvas = torch.cat([pixels, colors], dim=2).squeeze(0).cpu().numpy()
                canvas = (canvas * 255).astype(np.uint8)
                imageio.imwrite(f"{self.render_dir}/val_step{step}_{i:04d}.png", canvas)

                pixels_p = pixels.permute(0, 3, 1, 2)
                colors_p = colors.permute(0, 3, 1, 2)
                metrics["psnr"].append(self.psnr(colors_p, pixels_p))
                metrics["ssim"].append(self.ssim(colors_p, pixels_p))
                metrics["lpips"].append(self.lpips(colors_p, pixels_p))

        if self.world_rank == 0:
            stats = {k: torch.stack(v).mean().item() for k, v in metrics.items()}
            stats["render_time"] = np.mean(render_times)
            stats["num_gaussians"] = len(self.splats["means"])

            print(
                f"[Eval] PSNR: {stats['psnr']:.3f}, "
                f"SSIM: {stats['ssim']:.4f}, "
                f"LPIPS: {stats['lpips']:.4f}, "
                f"Time: {stats['render_time']:.3f}s"
            )

            with open(f"{self.stats_dir}/val_step{step}.json", "w") as f:
                json.dump(stats, f)

            for k, v in stats.items():
                self.writer.add_scalar(f"val/{k}", v, step)
            self.writer.flush()

    @torch.no_grad()
    def render_trajectory(self, step: int, n_frames: int = 100):
        """Render a trajectory video across time."""
        print(f"[FreeTimeGS] Rendering trajectory at step {step}...")

        cfg = self.cfg
        device = self.device

        # Use first camera pose
        data = self.valset[0]
        camtoworld = data["camtoworld"].unsqueeze(0).to(device)
        K = data["K"].unsqueeze(0).to(device)
        height, width = data["image"].shape[:2]
        height = height // cfg.data_factor
        width = width // cfg.data_factor

        video_dir = f"{cfg.result_dir}/videos"
        os.makedirs(video_dir, exist_ok=True)
        writer = imageio.get_writer(f"{video_dir}/trajectory_step{step}.mp4", fps=30)

        for i in tqdm.trange(n_frames, desc="Rendering trajectory"):
            t = i / (n_frames - 1)  # Time from 0 to 1

            colors, _, _ = self.rasterize_at_time(
                camtoworlds=camtoworld,
                Ks=K,
                width=width,
                height=height,
                t=t,
                sh_degree=cfg.sh_degree,
                near_plane=cfg.near_plane,
                far_plane=cfg.far_plane,
            )

            colors = torch.clamp(colors[..., :3], 0.0, 1.0)
            frame = (colors.squeeze(0).cpu().numpy() * 255).astype(np.uint8)
            writer.append_data(frame)

        writer.close()
        print(f"[FreeTimeGS] Video saved to {video_dir}/trajectory_step{step}.mp4")


def run_roma_triangulation(cfg: FreeTimeConfig):
    """Run RoMaV2 triangulation before training if --run_roma is enabled."""
    import subprocess

    print("\n" + "=" * 80)
    print("[RoMaV2] Running triangulation before training...")
    print("=" * 80)

    # Build the triangulate command
    cmd = [
        "python", "triangulate_romav2.py",
        "--cam_dir", os.path.join(cfg.data_dir, "sparse/0"),
        "--frame_start", str(cfg.start_frame),
        "--frame_end", str(cfg.end_frame),
        "--frame_step", str(cfg.frame_step),
        "--num_matches", str(cfg.roma_num_matches),
        "--max_pairs", str(cfg.roma_max_pairs),
        "--setting", cfg.roma_setting,
        "--output_dir", os.path.join(cfg.data_dir, "sparse"),
    ]

    print(f"[RoMaV2] Command: {' '.join(cmd)}")

    try:
        result = subprocess.run(cmd, check=True, capture_output=False)
        print("[RoMaV2] Triangulation completed successfully!")
    except subprocess.CalledProcessError as e:
        print(f"[RoMaV2] WARNING: Triangulation failed with exit code {e.returncode}")
        print("[RoMaV2] Continuing with existing sparse models...")
    except FileNotFoundError:
        print("[RoMaV2] WARNING: triangulate_romav2.py not found")
        print("[RoMaV2] Continuing with existing sparse models...")

    print("=" * 80 + "\n")


def main(local_rank: int, world_rank: int, world_size: int, cfg: FreeTimeConfig):
    """Main entry point."""

    # Run RoMaV2 triangulation if enabled (only on rank 0)
    if cfg.run_roma and local_rank == 0:
        run_roma_triangulation(cfg)

    runner = FreeTimeGSRunner(local_rank, world_rank, world_size, cfg)
    runner.train()

    # Render final trajectory
    runner.render_trajectory(step=cfg.max_steps - 1)


if __name__ == "__main__":
    import argparse

    # Check for --single-gpu flag before tyro parsing
    pre_parser = argparse.ArgumentParser(add_help=False)
    pre_parser.add_argument("--single-gpu", action="store_true",
                           help="Run on single GPU (no distributed training)")
    pre_args, remaining = pre_parser.parse_known_args()

    # Remove --single-gpu from sys.argv so tyro doesn't see it
    if pre_args.single_gpu:
        sys.argv = [sys.argv[0]] + remaining

    configs = {
        "default": (
            "FreeTimeGS training with default settings.",
            FreeTimeConfig(),
        ),
        "fast": (
            "Fast training with lower quality for testing.",
            FreeTimeConfig(
                max_steps=7_000,
                eval_steps=[7_000],
                save_steps=[7_000],
                data_factor=4,
                max_init_points=100_000,  # Limit points to avoid OOM
            ),
        ),
        "full": (
            "Full training with relocation only (paper's approach).",
            FreeTimeConfig(
                max_steps=30_000,
                eval_steps=[7_000, 15_000, 30_000],
                save_steps=[7_000, 15_000, 30_000],
                data_factor=2,  # Higher resolution (factor=1 for full res)
                max_init_points=200_000,  # More initial points
                use_default_strategy=False,  # Paper uses relocation only, NO densification/pruning
                use_periodic_relocation=True,
                relocation_every=100,  # Paper: every 100 iterations
            ),
        ),
    }

    cfg = tyro.extras.overridable_config_cli(configs)

    if pre_args.single_gpu:
        # Single GPU mode: run directly without distributed wrapper
        print("[FreeTimeGS] Running in single-GPU mode")
        main(local_rank=0, world_rank=0, world_size=1, cfg=cfg)
    else:
        # Multi-GPU distributed mode
        cli(main, cfg, verbose=True)
